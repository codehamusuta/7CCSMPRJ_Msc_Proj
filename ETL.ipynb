{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6718b38-f9f3-4e4c-9196-d2f469e5adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5651af8-42e2-4e26-8fd7-55c61319b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../data_2023_06_02'\n",
    "climate_raw = os.path.join(root, 'raw/CLIMATE-FEVER')\n",
    "climate_out = os.path.join(root, 'preprocessed/CLIMATE-FEVER')\n",
    "\n",
    "pubhealth_raw = os.path.join(root, 'raw/PUBHEALTH')\n",
    "pubhealth_out = os.path.join(root, 'preprocessed/PUBHEALTH')\n",
    "\n",
    "fever_raw = os.path.join(root, 'raw/FEVER')\n",
    "fever_out = os.path.join(root, 'preprocessed/FEVER')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f051749-2d2d-4eef-b67d-91c395668e7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Climate Fever\n",
    "- drop DISPUTED; only keep SUPPORTS, REFUTES and NOT_ENOUGH_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "768d1c29-e6aa-46df-8c82-3507217902b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1535, 4)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "with open(os.path.join(climate_raw, 'climate-fever-dataset-r1.jsonl'), 'r') as f:\n",
    "    climate_lst = [json.loads(item) for item in list(f)]\n",
    "\n",
    "climate_df = pd.DataFrame.from_records(climate_lst)\n",
    "print(climate_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5bec6ec8-f4a7-4dfd-b84b-b361d16f8fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SUPPORTS           654\n",
       "NOT_ENOUGH_INFO    474\n",
       "REFUTES            253\n",
       "DISPUTED           154\n",
       "Name: claim_label, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_df['claim_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45934fc9-f965-4c30-ac8b-c62238e74f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1381, 4)\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "climate_df = climate_df[climate_df['claim_label'] != 'DISPUTED'].copy()\n",
    "print(climate_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f30fe28-db65-4c38-a16b-02a303ad01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "if not os.path.isdir(climate_out):\n",
    "    os.mkdir(climate_out)\n",
    "with open(os.path.join(climate_out, 'climate-fever.jsonl'), 'w') as f:\n",
    "    f.write(climate_df.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40169982-22da-4553-af69-d846c4a45e02",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PubHealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312dc974-bff6-483a-9b6d-269f07dbc473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21193529/conda/jenv3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef35fe-daca-478d-a3d3-a7bd372dc373",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4265158-308a-41e0-813b-8be7700057f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9832, 9) (1221, 9) (1235, 10)\n"
     ]
    }
   ],
   "source": [
    "pub_train_df = pd.read_csv(os.path.join(pubhealth_raw, 'train.tsv'), sep=\"\\t\")\n",
    "pub_dev_df = pd.read_csv(os.path.join(pubhealth_raw, 'dev.tsv'), sep=\"\\t\")\n",
    "pub_test_df = pd.read_csv(os.path.join(pubhealth_raw, 'test.tsv'), sep=\"\\t\")\n",
    "\n",
    "print(pub_train_df.shape, pub_dev_df.shape, pub_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae82cf91-f92f-43bb-b92b-5b2d799e8b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1235, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_test_df[pub_test_df['main_text'].notnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1589c0f-c20f-452a-b3c1-7ecbaf31d456",
   "metadata": {
    "tags": []
   },
   "source": [
    "### transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3809551b-906d-4a25-848e-af7f8595c4df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### filter\n",
    "- drop data points w/o main text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8c45d50-4c03-4f65-9cd6-68b35aa4e3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9806, 9) (1217, 9) (1235, 10)\n"
     ]
    }
   ],
   "source": [
    "pub_train_df = pub_train_df[pub_train_df['main_text'].notnull()].copy()\n",
    "pub_dev_df = pub_dev_df[pub_dev_df['main_text'].notnull()].copy()\n",
    "pub_test_df = pub_test_df[pub_test_df['main_text'].notnull()].copy()\n",
    "\n",
    "print(pub_train_df.shape, pub_dev_df.shape, pub_test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0955753a-ddf5-4b98-9d0c-dd8911c92837",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### evidence selection\n",
    "- select top 5 evidence sentences from main_text using SBERT (https://github.com/neemakot/Health-Fact-Checking/blob/master/src/load_data.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85bc6c2d-2d25-44cf-8606-fdcb499b08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_evidence_sentences(df, k=5):\n",
    "    \"\"\"Select top k evidence sentences based on sentence transformer model\"\"\"\n",
    "\n",
    "    corpus = df.copy()\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    model.to(\"cuda\")\n",
    "    corpus['top_k'] = np.empty([len(corpus),], dtype=str)\n",
    "\n",
    "    for index, row in corpus.iterrows():\n",
    "        claim = row['claim']\n",
    "        sentences = [claim] + [sent for sent in sent_tokenize(row['main_text'])]\n",
    "        \n",
    "        sentence_embeddings = sentence_transformer_model.encode(sentences)\n",
    "        claim_embedding = sentence_embeddings[0]\n",
    "        sentence_embeddings = sentence_embeddings[1:]\n",
    "        cosine_similarity_emb = {}\n",
    "        \n",
    "        for sent, embedding in zip(sentences, sentence_embeddings):\n",
    "            cosine_similarity_emb[sent] = np.linalg.norm(cosine_similarity(\n",
    "                [claim_embedding, embedding]))\n",
    "            \n",
    "        top_k = dict(sorted(cosine_similarity_emb.items(),\n",
    "                            key=itemgetter(1))[:k])\n",
    "        # corpus.at[index, 'top_k'] = ','.join(key for key in top_k.keys())\n",
    "        corpus.at[index, 'top_k'] = [key for key in top_k.keys()]\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ca88d00-8bdd-49ae-b994-dfba5020f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_train_df = select_evidence_sentences(pub_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b740978f-0899-4085-a2cc-74d6c2a9b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_dev_df = select_evidence_sentences(pub_dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a76d901-6052-4c3b-a0cc-7931d6045f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_test_df = select_evidence_sentences(pub_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1094b1e-4470-4e5a-829b-362245784eb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cb67e6e-5c87-438f-8fc1-a47b10b95331",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(pubhealth_out):\n",
    "    os.mkdir(pubhealth_out)\n",
    "with open(os.path.join(pubhealth_out, 'train.jsonl'), 'w') as f:\n",
    "    f.write(pub_train_df.to_json(orient='records', lines=True))\n",
    "    \n",
    "with open(os.path.join(pubhealth_out, 'dev.jsonl'), 'w') as f:\n",
    "    f.write(pub_dev_df.to_json(orient='records', lines=True))\n",
    "    \n",
    "with open(os.path.join(pubhealth_out, 'test.jsonl'), 'w') as f:\n",
    "    f.write(pub_test_df.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c93c2-8f4b-4376-b427-bcfd2185018c",
   "metadata": {},
   "source": [
    "## FEVER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6601d2-e001-43b3-9041-fb48ad92f7a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### load fever data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4eaf3082-f372-4e53-b20e-e6884941d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145449 item loaded\n",
      "9999 item loaded\n",
      "9999 item loaded\n"
     ]
    }
   ],
   "source": [
    "# read as dataframe\n",
    "def load_data(jsonl_file):\n",
    "    with open(jsonl_file, 'r') as f:\n",
    "        fever_lst = [json.loads(item) for item in list(f)]\n",
    "        print(f'{len(fever_lst)} item loaded')\n",
    "        df = pd.DataFrame.from_records(fever_lst)\n",
    "    return df\n",
    "\n",
    "# These are original data files\n",
    "# fever_train_df = load_data(os.path.join(fever_raw, 'train.jsonl'))\n",
    "# fever_dev_df = load_data(os.path.join(fever_raw, 'paper_dev.jsonl'))\n",
    "# fever_test_df = load_data(os.path.join(fever_raw, 'paper_test.jsonl'))\n",
    "\n",
    "# There are original data files + random sampled evidence for NOT ENOUGH INFO class\n",
    "fever_train_df = load_data(os.path.join(fever_raw, 'train.ns.rand.jsonl'))\n",
    "fever_dev_df = load_data(os.path.join(fever_raw, 'dev.ns.rand.jsonl'))\n",
    "fever_test_df = load_data(os.path.join(fever_raw, 'test.ns.rand.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce1528-83c5-4ac2-85ff-732c9285b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read as dict\n",
    "\n",
    "# import csv\n",
    "# import json\n",
    "\n",
    "# class Reader:\n",
    "#     def __init__(self,encoding=\"utf-8\"):\n",
    "#         self.enc = encoding\n",
    "\n",
    "#     def read(self,file):\n",
    "#         with open(file,\"r\",encoding = self.enc) as f:\n",
    "#             return self.process(f)\n",
    "\n",
    "#     def process(self,f):\n",
    "#         pass\n",
    "\n",
    "# class JSONLineReader(Reader):\n",
    "#     def process(self,fp):\n",
    "#         data = []\n",
    "#         for line in fp.readlines():\n",
    "#             data.append(json.loads(line.strip()))\n",
    "#         return data\n",
    "    \n",
    "# reader = JSONLineReader()\n",
    "# t = reader.read(os.path.join(fever_raw, 'test.ns.rand.jsonl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aff745-0da1-47c1-94cf-5bf78cfd9cd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### extract evidence text from fever.db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e043a-d0ce-4e93-aff7-a7a4a5e18066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Questions\n",
    "- do i combine the evidence text? (src/rte/parikh/reader.py --> FEVERReader) (src/retrieval/reader.py --> FEVERSentenceReader)\n",
    "    - \" \".join(evidences)\n",
    "- how to extract evidence text for NA cases? - PYTHONPATH=src python src/scripts/dataset/neg_sample_evidence.py data/fever/fever.db\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4acd0e28-3b43-497e-89e6-5af8f3fcaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference 1: https://github.com/facebookresearch/DrQA/blob/main/drqa/retriever/doc_db.py\n",
    "# reference 2: src/retrieval/fever_doc_db.py\n",
    "\n",
    "import unicodedata\n",
    "import sqlite3\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"Resolve different type of unicode encodings.\"\"\"\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "\n",
    "class DocDB():\n",
    "    \"\"\"Sqlite backed document storage.\n",
    "\n",
    "    Implements get_doc_text(doc_id).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, db_path=None):\n",
    "        self.path = db_path or DEFAULTS['db_path']\n",
    "        self.connection = sqlite3.connect(self.path, check_same_thread=False)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.close()\n",
    "\n",
    "    def path(self):\n",
    "        \"\"\"Return the path to the file that backs this database.\"\"\"\n",
    "        return self.path\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the connection to the database.\"\"\"\n",
    "        self.connection.close()\n",
    "\n",
    "    def get_doc_ids(self):\n",
    "        \"\"\"Fetch all ids of docs stored in the db.\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(\"SELECT id FROM documents\")\n",
    "        results = [r[0] for r in cursor.fetchall()]\n",
    "        cursor.close()\n",
    "        return results\n",
    "\n",
    "    def get_doc_text(self, doc_id):\n",
    "        \"\"\"Fetch the raw text of the doc for 'doc_id'.\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(\n",
    "            \"SELECT text FROM documents WHERE id = ?\",\n",
    "            (normalize(doc_id),)\n",
    "        )\n",
    "        result = cursor.fetchone()\n",
    "        cursor.close()\n",
    "        return result if result is None else result[0]\n",
    "    \n",
    "    def get_doc_lines(self, doc_id):\n",
    "        \"\"\"Fetch the raw text of the doc for 'doc_id'.\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(\n",
    "            \"SELECT lines FROM documents WHERE id = ?\",\n",
    "            (normalize(doc_id),)\n",
    "        )\n",
    "        result = cursor.fetchone()\n",
    "        cursor.close()\n",
    "        return result if result is None else result[0]\n",
    "    \n",
    "    def get_non_empty_doc_ids(self):\n",
    "        \"\"\"Fetch all ids of docs stored in the db.\"\"\"\n",
    "        cursor = self.connection.cursor()\n",
    "        cursor.execute(\"SELECT id FROM documents WHERE length(trim(text)) > 0\")\n",
    "        results = [r[0] for r in cursor.fetchall()]\n",
    "        cursor.close()\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c59b48c-e210-449b-ab65-5e579b341324",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocDB(os.path.join(fever_raw, 'fever.db'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667aa1c7-34e4-43ae-8b5f-179b65acc0cf",
   "metadata": {},
   "source": [
    "#### Extract evidence text for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d17dc121-a3ed-44c8-9333-d2604d1cdae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class SimpleRandom():\n",
    "    instance = None\n",
    "\n",
    "    def __init__(self,seed):\n",
    "        self.seed = seed\n",
    "        self.random = random.Random(seed)\n",
    "\n",
    "    def next_rand(self,a,b):\n",
    "        return self.random.randint(a,b)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_instance():\n",
    "        if SimpleRandom.instance is None:\n",
    "            SimpleRandom.instance = SimpleRandom(SimpleRandom.get_seed())\n",
    "        return SimpleRandom.instance\n",
    "\n",
    "    @staticmethod\n",
    "    def get_seed():\n",
    "        return int(os.getenv(\"RANDOM_SEED\", 12459))\n",
    "\n",
    "    @staticmethod\n",
    "    def set_seeds():\n",
    "        np.random.seed(SimpleRandom.get_seed())\n",
    "        random.seed(SimpleRandom.get_seed())\n",
    "\n",
    "def get_doc_line(doc,line):\n",
    "    lines = db.get_doc_lines(doc)\n",
    "    if line > -1:\n",
    "        return lines.split(\"\\n\")[line].split(\"\\t\")[1] #get specific line in wiki page\n",
    "    else:\n",
    "        non_empty_lines = [line.split(\"\\t\")[1] for line in lines.split(\"\\n\") if len(line.split(\"\\t\"))>1 and len(line.split(\"\\t\")[1].strip())]\n",
    "        return non_empty_lines[SimpleRandom.get_instance().next_rand(0,len(non_empty_lines)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "24d426e6-e1a0-4d8c-a277-fbcfe2ec8e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_evidence(evidence):\n",
    "    try:\n",
    "        pages = []\n",
    "        for evidence_group in evidence:\n",
    "            pages.extend([(ev[2],ev[3]) for ev in evidence_group])\n",
    "        \n",
    "        lines = set([get_doc_line(d[0],d[1]) for d in pages])\n",
    "        premise = \" \".join(lines)\n",
    "        return lines\n",
    "    except Exception as e:\n",
    "        print(evidences)\n",
    "        print(e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f529e1c-e1ed-45db-b6ed-304eac8f327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fever_test_df['premise'] = fever_test_df['evidence'].apply(extract_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "143699e3-92fc-4161-9a12-121f22c41e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fever_dev_df['premise'] = fever_dev_df['evidence'].apply(extract_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "374f98f0-987d-4c8e-9f13-d80e17d00099",
   "metadata": {},
   "outputs": [],
   "source": [
    "fever_train_df['premise'] = fever_train_df['evidence'].apply(extract_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fcdf3-d61d-47ef-81f1-e2f222444370",
   "metadata": {},
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9b80d31a-8b5c-4dc6-a9d1-9a52f6d98dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(fever_out):\n",
    "    os.mkdir(fever_out)\n",
    "with open(os.path.join(fever_out, 'train_preprocessed.ns.rand.jsonl'), 'w') as f:\n",
    "    f.write(fever_train_df.to_json(orient='records', lines=True))\n",
    "    \n",
    "with open(os.path.join(fever_out, 'dev_preprocessed.ns.rand.jsonl'), 'w') as f:\n",
    "    f.write(fever_dev_df.to_json(orient='records', lines=True))\n",
    "    \n",
    "with open(os.path.join(fever_out, 'test_preprocessed.ns.rand.jsonl'), 'w') as f:\n",
    "    f.write(fever_test_df.to_json(orient='records', lines=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

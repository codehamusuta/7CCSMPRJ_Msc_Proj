{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdd5615-b0a0-4cb4-9a57-2cfa2a780581",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3622b96f-f89a-4d89-aa1c-dca4d9ab5eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21193529/conda/jenv3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab123aaf-3afb-4438-ae54-ea918895ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3893197c-5151-4c70-a70e-ab7ab03ccfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, ClassLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464fa093-0a8c-4c36-85b8-8727c617987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 28.8kB [00:00, 20.0MB/s]\n",
      "Downloading metadata: 28.7kB [00:00, 25.0MB/s]\n",
      "Downloading readme: 27.9kB [00:00, 31.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mnli to /users/k21193529/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313M/313M [00:01<00:00, 159MB/s] \n",
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /users/k21193529/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 666.57it/s]\n",
      "/tmp/ipykernel_43833/3241208262.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", task)\n",
      "Downloading builder script: 5.76kB [00:00, 5.86MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "task = \"mnli\"\n",
    "dataset = load_dataset(\"glue\", task)\n",
    "metric = load_metric(\"glue\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b90fb18-9fd9-4a60-90df-838484652b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "962c3cc1-81b7-43a0-959a-7381fd49b98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': Value(dtype='string', id=None),\n",
       " 'hypothesis': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da3e99b8-cadd-458c-a977-2e6cff7c75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) -1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28d4e3a9-af5f-48a0-9d2e-685c4179761d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haynes Johnson ( NewsHour ) credits him with bringing the South and Sunbelt into the GOP.</td>\n",
       "      <td>Haynes Johnson thought he changed the political opinions of southerners.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>195422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What time did you go out last evening?</td>\n",
       "      <td>What time did you go out last night? 8?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>218361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When you reach an impressive plaza with a 16th-century church and whitewashed town hall, you may feel that you've come to the top of the town but you haven't.</td>\n",
       "      <td>You haven't come to the top of the town when you reach an impressive plaza with a 16th-century church.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>359426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you think they should in in that profession</td>\n",
       "      <td>Do you believe they should in that job?</td>\n",
       "      <td>entailment</td>\n",
       "      <td>107527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Even if it were, we couldn't afford it.</td>\n",
       "      <td>Even if it were, we'd easily be able to afford it.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>32591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Set aside money specifically for planning.</td>\n",
       "      <td>Save money for planning.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>108719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>But there is one place where Will's journalism does seem to matter, where he does toss  baseball.</td>\n",
       "      <td>Will can't do anything if it has nothing to do with baseball</td>\n",
       "      <td>neutral</td>\n",
       "      <td>294315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Straits Settlements were formed after the Anglo-Dutch Treaty of London (1824).</td>\n",
       "      <td>A treaty between the English and Dutch enabled the creation of the Straits Settlements.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>223306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i uh for the longest time i i'd gotten rid of my gas credit cards and then all of a sudden i started getting a flurry of these things so i i did i have picked up three gas credit cards in the last couple months</td>\n",
       "      <td>Gas credit cards have helped my situation immensely</td>\n",
       "      <td>neutral</td>\n",
       "      <td>199554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Life was about to get very difficult until Adrin and San'doro came back to him.</td>\n",
       "      <td>Adrin and San'doro made things harder for him.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>321656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "898efb7b-4969-44fc-818f-b5e8dac491a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d3dd96-74e9-401b-a3b6-c58c18d8152c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.0/28.0 [00:00<00:00, 6.64kB/s]\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:00<00:00, 127kB/s]\n",
      "Downloading (â€¦)solve/main/vocab.txt: 232kB [00:00, 11.0MB/s]\n",
      "Downloading (â€¦)/main/tokenizer.json: 466kB [00:00, 8.08MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "task = \"mnli\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdd4ec60-acd3-4920-a22e-657dcd39af8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can directly call tokeknizer on one sentence or pair of sentence\n",
    "tokenizer(\"hello, this is one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a39d707-ee92-472e-b15b-22e36add2713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello, this is one sentence! [SEP] and this sentence goes with it. [SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(\"hello, this is one sentence!\", \"And this sentence goes with it.\")['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0211f72e-467a-4e11-aff0-d8947354acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task key for mnli\n",
    "key1 = \"premise\"\n",
    "key2 = \"hypothesis\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[key1], examples[key2], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a39e1261-5adf-4b44-8bd4-71f6afb67d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "encoded_ds = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f301145-cd13-4c8c-af94-6a2d662f8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5266f349-04de-4d85-9e7f-d70b09136817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:02<00:00, 121MB/s]  \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = 3 # mnli has 3 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c86d409-3d46-45ba-8f31-61d10af10c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5, \n",
    "    per_device_train_batch_size = batch_size, \n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    num_train_epochs = 5, \n",
    "    weight_decay = 0.01, \n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45a50767-e9b4-4cec-b606-468febdac28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions = predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "975686f9-9620-4fa5-abfa-1ff9ed3ae0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_key = \"validation_matched\"\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    args,\n",
    "    train_dataset = encoded_ds[\"train\"],\n",
    "    eval_dataset = encoded_ds[validation_key],\n",
    "    tokenizer = tokenizer, \n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6688736-5e42-43d8-a375-be0afae49c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 392702\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 122720\n",
      "  Number of trainable parameters = 66955779\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='122720' max='122720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [122720/122720 1:13:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>0.561836</td>\n",
       "      <td>0.815894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.561836</td>\n",
       "      <td>0.815894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.220400</td>\n",
       "      <td>0.561836</td>\n",
       "      <td>0.815894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.212700</td>\n",
       "      <td>0.561836</td>\n",
       "      <td>0.815894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.561836</td>\n",
       "      <td>0.815894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-mnli/checkpoint-24544\n",
      "Configuration saved in distilbert-base-uncased-finetuned-mnli/checkpoint-24544/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-mnli/checkpoint-24544/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-mnli/checkpoint-24544/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-mnli/checkpoint-24544/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-mnli/checkpoint-49088\n",
      "Configuration saved in distilbert-base-uncased-finetuned-mnli/checkpoint-49088/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-mnli/checkpoint-49088/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-mnli/checkpoint-49088/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-mnli/checkpoint-49088/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-mnli/checkpoint-73632\n",
      "Configuration saved in distilbert-base-uncased-finetuned-mnli/checkpoint-73632/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-mnli/checkpoint-73632/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-mnli/checkpoint-73632/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-mnli/checkpoint-73632/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-mnli/checkpoint-98176\n",
      "Configuration saved in distilbert-base-uncased-finetuned-mnli/checkpoint-98176/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-mnli/checkpoint-98176/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-mnli/checkpoint-98176/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-mnli/checkpoint-98176/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-mnli/checkpoint-122720\n",
      "Configuration saved in distilbert-base-uncased-finetuned-mnli/checkpoint-122720/config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-mnli/checkpoint-122720/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-mnli/checkpoint-122720/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-mnli/checkpoint-122720/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distilbert-base-uncased-finetuned-mnli/checkpoint-24544 (score: 0.8158940397350993).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=122720, training_loss=0.21243214613455683, metrics={'train_runtime': 4394.003, 'train_samples_per_second': 446.861, 'train_steps_per_second': 27.929, 'total_flos': 4.122893008518235e+16, 'train_loss': 0.21243214613455683, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9f4548e-d0b8-4c7b-a4a9-82031ce92e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise. If idx, hypothesis, premise are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='614' max='614' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [614/614 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5618363618850708,\n",
       " 'eval_accuracy': 0.8158940397350993,\n",
       " 'eval_runtime': 6.3631,\n",
       " 'eval_samples_per_second': 1542.497,\n",
       " 'eval_steps_per_second': 96.494,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00b5e4-3188-4138-83eb-0f8442ffa0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08121def-b144-4ff4-a101-4801033801bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/k21193529/conda/jenv3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from main import load_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33bbc1c-2cd1-4e31-9c47-10c9d7871c35",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d9e765-9412-46e0-98d8-41bc7bcf5db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../data_2023_06_02'\n",
    "\n",
    "fever_dir = os.path.join(root, 'preprocessed/FEVER')\n",
    "pubhealth_dir = os.path.join(root, 'preprocessed/PUBHEALTH')\n",
    "climate_dir = os.path.join(root, 'preprocessed/CLIMATE-FEVER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5564286-8466-48ae-a067-00bb00f39610",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fever, ds_pubhealth, ds_climate, ds_test = load_datasets(fever_dir, pubhealth_dir, climate_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f0422-4074-4cbb-bd9e-f012c3d36cea",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "750c1fca-d910-4dfd-b825-b5d5c1b0afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT\n",
    "# model_checkpoint = \"../models/BERT_FEVER/checkpoint-4546\"\n",
    "# model_checkpoint = \"../models/BERT_PUBHEALTH/checkpoint-262\"\n",
    "model_checkpoint = \"../models/BERT_CLIMATE/checkpoint-62\"\n",
    "\n",
    "#RoBERT\n",
    "# model_checkpoint = \"../models/RoBERTa_FEVER/checkpoint-2273\"\n",
    "# model_checkpoint = \"../models/RoBERTa_PUBHEALTH/checkpoint-262\"\n",
    "# model_checkpoint = \"../models/RoBERTa_CLIMATE/checkpoint-93\"\n",
    "\n",
    "\n",
    "#SciBERT\n",
    "# model_checkpoint = \"../models/SciBERT_FEVER/checkpoint-4546\"\n",
    "# model_checkpoint = \"../models/SciBERT_PUBHEALTH/checkpoint-131\"\n",
    "# model_checkpoint = \"../models/SciBERT_CLIMATE/checkpoint-31\"\n",
    "\n",
    "#BioBERT\n",
    "# model_checkpoint = \"../models/BioBERT_FEVER/best_model\"\n",
    "# model_checkpoint = \"../models/BioBERT_PUBHEALTH/best_model\"\n",
    "# model_checkpoint = \"../models/BioBERT_CLIMATE/best_model\"\n",
    "\n",
    "#ALBERT\n",
    "# model_checkpoint = \"../models/ALBERT_FEVER/best_model\"\n",
    "# model_checkpoint = \"../models/ALBERT_PUBHEALTH/best_model\"\n",
    "# model_checkpoint = \"../models/ALBERT_CLIMATE/best_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a72a4-f7b5-40d8-8818-5b514c52c703",
   "metadata": {},
   "source": [
    "## Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b979b0f5-412d-4ba0-8d43-4328dcc7ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "\n",
    "def _evaluate(model, ds, device, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (pytorch model): model to evaluate\n",
    "        ds (torch.DataLoader): dataset to evaluate on loaded into pytorch DataLoader obj\n",
    "        device (torch.device): GPU / CPU\n",
    "        metric (string): evaluation metrics to use. Defaults to accuracy.\n",
    "    \"\"\"\n",
    "    metric = evaluate.load(metric)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for batch in ds:\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=preds, references=batch[\"labels\"])\n",
    "        predictions = predictions + preds.tolist()\n",
    "        \n",
    "    predictions = np.array(predictions)\n",
    "    metric_val = metric.compute()\n",
    "\n",
    "    return metric_val, predictions\n",
    "\n",
    "def _get_misclassified_samples(ds, predictions):\n",
    "    df = pd.DataFrame(ds)\n",
    "    df['pred'] = predictions\n",
    "    df['misclassified'] = df['label'] != df['pred']\n",
    "    print(df.groupby('label')['misclassified'].value_counts(normalize=True)*100) #misclassified = True\n",
    "    return df\n",
    "\n",
    "def evaluate_model(model_checkpoint, ds_test, metric=\"accuracy\"):\n",
    "    \"\"\"Evaluate accuracy of saved model on test datasets\n",
    "    \n",
    "    Args:\n",
    "        mdoel_checkpoint (string): path to best model,\n",
    "        ds_test (DatasetDict): huggingface dataset for fever_test, pubhealth_test, climate_test,\n",
    "        metric (string): evaluation metrics to use. Defaults to accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    #===================================================\n",
    "    # Load Model\n",
    "    #===================================================\n",
    "    num_labels = 3 \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(f\"Model loaded into {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    #===================================================\n",
    "    # Tokenize dataset\n",
    "    #===================================================\n",
    "    print(f\"Tokenizing dataset\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    def preprocess_function(samples):\n",
    "        return tokenizer(samples['claim'], samples['evidence'], \n",
    "                         padding=True,\n",
    "                         truncation='only_second', \n",
    "                         max_length=512)\n",
    "\n",
    "    encoded_ds = ds_test.map(preprocess_function, batched=True)\n",
    "\n",
    "    # format tokens to fit huggingface language model formats\n",
    "    encoded_ds = encoded_ds.remove_columns([\"claim\", \"evidence\"])\n",
    "    encoded_ds = encoded_ds.rename_column(\"label\", \"labels\")\n",
    "    encoded_ds.set_format(\"torch\")\n",
    "\n",
    "    #===================================================\n",
    "    # Evaluate\n",
    "    #===================================================    \n",
    "    for ds_name in encoded_ds.keys():\n",
    "        print(f\"Evaluating {ds_name}\")\n",
    "        eval_ds = DataLoader(encoded_ds[ds_name], batch_size=8)\n",
    "        r, predictions = _evaluate(model, eval_ds, device, metric)\n",
    "        print(f\"Overall Accuracy for {ds_name} :: {r}\")\n",
    "\n",
    "        df = _get_misclassified_samples(ds_test[ds_name], predictions)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "9abb0d63-1652-4d1d-83ca-f020e0c4ca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded into cuda\n",
      "Tokenizing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fever\n",
      "Overall Accuracy for fever :: {'accuracy': 0.6060606060606061}\n",
      "label  misclassified\n",
      "0      False            81.818182\n",
      "       True             18.181818\n",
      "1      True             99.939994\n",
      "       False             0.060006\n",
      "2      False            99.939994\n",
      "       True              0.060006\n",
      "Name: misclassified, dtype: float64\n",
      "Evaluating pubhealth\n",
      "Overall Accuracy for pubhealth :: {'accuracy': 0.3953488372093023}\n",
      "label  misclassified\n",
      "0      False            65.275459\n",
      "       True             34.724541\n",
      "1      True             99.742268\n",
      "       False             0.257732\n",
      "2      True             64.444444\n",
      "       False            35.555556\n",
      "Name: misclassified, dtype: float64\n",
      "Evaluating climate\n",
      "Overall Accuracy for climate :: {'accuracy': 0.575}\n",
      "label  misclassified\n",
      "0      False            78.947368\n",
      "       True             21.052632\n",
      "1      True             88.888889\n",
      "       False            11.111111\n",
      "2      False            52.173913\n",
      "       True             47.826087\n",
      "Name: misclassified, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_checkpoint, ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaeba1d-b177-48b5-87c8-6816dea43aeb",
   "metadata": {},
   "source": [
    "### get statistics of data distribution by labels for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d942b11-489f-4313-8ff4-bbe9f66fd483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_fever, ds_pubhealth, ds_climate, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "576b1ced-dea5-47d1-8fe4-d1ac481c414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    33.333333\n",
      "0    33.333333\n",
      "1    33.333333\n",
      "Name: label, dtype: float64\n",
      "2    3333\n",
      "0    3333\n",
      "1    3333\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(ds_fever['fever_test'])\n",
    "print(df['label'].value_counts(normalize=True)*100)\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "89356cc0-266f-4034-a947-f8ec46a52c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    59.904762\n",
      "1    36.190476\n",
      "2     3.904762\n",
      "Name: label, dtype: float64\n",
      "0    629\n",
      "1    380\n",
      "2     41\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(ds_pubhealth['validation'])\n",
    "print(df['label'].value_counts(normalize=True)*100)\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4d97d56a-39f7-43f8-ba4e-f320ae1979a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    47.298675\n",
      "2    34.352701\n",
      "1    18.348624\n",
      "Name: label, dtype: float64\n",
      "0    464\n",
      "2    337\n",
      "1    180\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(ds_climate['train'])\n",
    "print(df['label'].value_counts(normalize=True)*100)\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "02c69fad-826e-46d8-ac23-1cc717802889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    47.5\n",
      "2    34.0\n",
      "1    18.5\n",
      "Name: label, dtype: float64\n",
      "0    95\n",
      "2    68\n",
      "1    37\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(ds_climate['validation'])\n",
    "print(df['label'].value_counts(normalize=True)*100)\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "859a6136-e2ba-45a9-9cf4-f66ded8e2d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    47.5\n",
      "2    34.5\n",
      "1    18.0\n",
      "Name: label, dtype: float64\n",
      "0    95\n",
      "2    69\n",
      "1    36\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(ds_test['climate'])\n",
    "print(df['label'].value_counts(normalize=True)*100)\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d1f64-22ae-42de-8a9e-4eb7fdb718b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
